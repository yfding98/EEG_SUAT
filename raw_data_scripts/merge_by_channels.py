#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
merge_by_channels.py

æ ¹æ® marked_abnormal_segments_postICA.csvï¼ŒæŒ‰å¼‚å¸¸é€šé“åˆ—è¡¨åˆ†ç»„åˆå¹¶æ•°æ®æ®µã€‚
ç›¸åŒå¼‚å¸¸é€šé“åˆ—è¡¨çš„æ‰€æœ‰ç‰‡æ®µï¼ˆä¸è®ºæ—¶é—´æ˜¯å¦è¿ç»­ï¼‰ä¼šè¢«åˆå¹¶åˆ°ä¸€ä¸ª .set æ–‡ä»¶ä¸­ã€‚

ä½¿ç”¨æ–¹æ³•:
    # å¤„ç†å•ä¸ªç»“æœç›®å½•
    python merge_by_channels.py --result_dir "E:\data\patient\SZ1_results"
    
    # æ‰¹é‡å¤„ç†æ‰€æœ‰ç»“æœç›®å½•
    python merge_by_channels.py --root_dir "E:\DataSet\EEG\EEG dataset_SUAT_processed"
"""

import os
import csv
import argparse
import numpy as np
import mne
from collections import defaultdict
from pathlib import Path


def read_marked_segments_postica(csv_path):
    """
    è¯»å– marked_abnormal_segments_postICA.csv
    
    è¿”å›: [(start, end, channels_str), ...]
    """
    segments = []
    
    if not os.path.exists(csv_path):
        print(f"âš  CSV file not found: {csv_path}")
        return segments
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        header = next(reader)  # è·³è¿‡è¡¨å¤´
        for row in reader:
            if len(row) >= 3:
                start = float(row[0])
                end = float(row[1])
                channels = row[2].strip()
                segments.append((start, end, channels))
    
    return segments


def group_segments_by_channels(segments):
    """
    æŒ‰å¼‚å¸¸é€šé“åˆ—è¡¨åˆ†ç»„
    
    è¿”å›: {channels_str: [(start, end), ...]}
    """
    grouped = defaultdict(list)
    
    for start, end, channels in segments:
        # æ ‡å‡†åŒ–é€šé“åˆ—è¡¨ï¼ˆæ’åºï¼Œç¡®ä¿ä¸€è‡´æ€§ï¼‰
        channel_list = sorted([ch.strip() for ch in channels.split(',')])
        channels_key = ','.join(channel_list)
        grouped[channels_key].append((start, end))
    
    # å¯¹æ¯ç»„å†…çš„æ—¶é—´æ®µæŒ‰å¼€å§‹æ—¶é—´æ’åº
    for channels_key in grouped:
        grouped[channels_key].sort(key=lambda x: x[0])
    
    return grouped


def merge_segments_for_channel_group(set_file, time_segments, channels_str, output_set_file):
    """
    åˆå¹¶æŒ‡å®šé€šé“ç»„çš„æ‰€æœ‰æ—¶é—´æ®µæ•°æ®
    
    å‚æ•°:
        set_file: è¾“å…¥çš„ _postICA.set æ–‡ä»¶è·¯å¾„
        time_segments: [(start, end), ...] æ—¶é—´æ®µåˆ—è¡¨ï¼ˆå·²æ’åºï¼‰
        channels_str: é€šé“åˆ—è¡¨å­—ç¬¦ä¸²ï¼Œç”¨äºæ³¨é‡Š
        output_set_file: è¾“å‡ºçš„ .set æ–‡ä»¶è·¯å¾„
    """
    # è¯»å– EEG æ•°æ®
    print(f"  Loading: {os.path.basename(set_file)}")
    raw = mne.io.read_raw_eeglab(set_file, preload=True, verbose='ERROR')
    sfreq = raw.info['sfreq']
    
    # æå–æ‰€æœ‰ç‰‡æ®µçš„æ•°æ®
    extracted_data_list = []
    segment_info = []
    
    for i, (start, end) in enumerate(time_segments):
        start_sample = int(start * sfreq)
        end_sample = int(end * sfreq)
        
        # ç¡®ä¿ä¸è¶Šç•Œ
        start_sample = max(0, start_sample)
        end_sample = min(len(raw.times), end_sample)
        
        if start_sample >= end_sample:
            print(f"    âš  Invalid segment: {start:.2f}-{end:.2f}s")
            continue
        
        # æå–æ•°æ®ç‰‡æ®µ
        segment_data = raw.get_data(start=start_sample, stop=end_sample)
        extracted_data_list.append(segment_data)
        segment_info.append((start, end, end_sample - start_sample))
        print(f"    Segment {i+1}: {start:.2f}s - {end:.2f}s ({end_sample - start_sample} samples)")
    
    if not extracted_data_list:
        print(f"    âš  No valid segments extracted")
        return False
    
    # æ‹¼æ¥æ‰€æœ‰ç‰‡æ®µ
    concatenated_data = np.concatenate(extracted_data_list, axis=1)
    total_samples = concatenated_data.shape[1]
    total_duration = total_samples / sfreq
    print(f"    âœ“ Merged {len(extracted_data_list)} segments â†’ {total_samples} samples ({total_duration:.2f}s)")
    
    # åˆ›å»ºæ–°çš„ Raw å¯¹è±¡
    info = raw.info.copy()
    new_raw = mne.io.RawArray(concatenated_data, info, verbose='ERROR')
    
    # æ·»åŠ æ³¨é‡Šè¯´æ˜æ¯ä¸ªç‰‡æ®µçš„æ¥æº
    descriptions = []
    onsets = []
    durations = []
    cumulative_time = 0
    
    for i, (orig_start, orig_end, n_samples) in enumerate(segment_info):
        descriptions.append(f"Original: {orig_start:.2f}s-{orig_end:.2f}s")
        onsets.append(cumulative_time)
        duration = n_samples / sfreq
        durations.append(duration)
        cumulative_time += duration
    
    annotations = mne.Annotations(onset=onsets, duration=durations, description=descriptions)
    new_raw.set_annotations(annotations)
    
    # ä¿å­˜ä¸º EEGLAB æ ¼å¼
    print(f"    Saving: {os.path.basename(output_set_file)}")
    mne.export.export_raw(output_set_file, new_raw, fmt='eeglab', overwrite=True, verbose='ERROR')
    print(f"    âœ“ Saved: {output_set_file}")
    
    return True


def process_single_result_dir(result_dir):
    """
    å¤„ç†å•ä¸ªç»“æœç›®å½•
    
    å‚æ•°:
        result_dir: ç»“æœç›®å½•è·¯å¾„ï¼ˆåŒ…å« marked_abnormal_segments_postICA.csvï¼‰
    """
    csv_path = os.path.join(result_dir, 'marked_abnormal_segments_postICA.csv')
    
    if not os.path.exists(csv_path):
        print(f"âš  marked_abnormal_segments_postICA.csv not found in: {result_dir}")
        print(f"  æç¤º: è¯·å…ˆè¿è¡Œ process_marked_segments.py ç”Ÿæˆæ­¤æ–‡ä»¶")
        return False
    
    # è·å–å¯¹åº”çš„ _postICA.set æ–‡ä»¶
    result_dir_name = os.path.basename(result_dir)
    if not result_dir_name.endswith('_results'):
        print(f"âš  Directory name doesn't end with _results: {result_dir}")
        return False
    
    set_name = result_dir_name.replace('_results', '.set')
    parent_dir = os.path.dirname(result_dir)
    set_file = os.path.join(parent_dir, set_name)
    
    if not os.path.exists(set_file):
        print(f"âš  SET file not found: {set_file}")
        return False
    
    print(f"\n{'='*80}")
    print(f"å¤„ç†: {result_dir_name}")
    print(f"{'='*80}")
    
    # è¯»å–æ ‡è®°çš„ç‰‡æ®µ
    segments = read_marked_segments_postica(csv_path)
    if not segments:
        print("âš  No segments found in CSV")
        return False
    
    print(f"æ€»å…± {len(segments)} ä¸ªæ ‡è®°ç‰‡æ®µ")
    
    # æŒ‰é€šé“åˆ†ç»„
    grouped = group_segments_by_channels(segments)
    print(f"æŒ‰é€šé“åˆ†ç»„å: {len(grouped)} ä¸ªä¸åŒçš„é€šé“ç»„åˆ\n")
    
    # ä¸ºæ¯ä¸ªé€šé“ç»„åˆ›å»ºåˆå¹¶çš„æ•°æ®æ–‡ä»¶
    success_count = 0
    statistics = {}
    
    for channels_str, time_segments in grouped.items():
        print(f"é€šé“ç»„: {channels_str}")
        print(f"  åŒ…å« {len(time_segments)} ä¸ªç‰‡æ®µ")
        
        # è®¡ç®—æ€»æ—¶é•¿
        total_duration = sum([end - start for start, end in time_segments])
        
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å
        # é€šé“åæ¸…ç†ï¼ˆå»é™¤ç‰¹æ®Šå­—ç¬¦ï¼‰
        safe_channels = channels_str.replace(',', '_').replace(' ', '').replace('-', '_')
        output_name = set_name.replace('.set', f'_merged_{safe_channels}.set')
        output_set_file = os.path.join(parent_dir, output_name)
        
        # åˆå¹¶æ•°æ®
        success = merge_segments_for_channel_group(
            set_file, time_segments, channels_str, output_set_file
        )
        
        if success:
            success_count += 1
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            statistics[channels_str] = {
                'count': len(time_segments),
                'total_duration': total_duration,
                'output_file': output_name
            }
        print()
    
    print(f"{'='*80}")
    print(f"âœ… å®Œæˆï¼æˆåŠŸåˆ›å»º {success_count}/{len(grouped)} ä¸ªåˆå¹¶æ–‡ä»¶")
    print(f"{'='*80}")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ° CSV
    if statistics:
        stats_csv_path = os.path.join(parent_dir, set_name.replace('.set', '_channel_merge_statistics.csv'))
        with open(stats_csv_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['é€šé“ç»„åˆ', 'ç‰‡æ®µæ•°é‡', 'æ€»æ—¶é•¿(ç§’)', 'æ€»æ—¶é•¿(åˆ†é’Ÿ)', 'è¾“å‡ºæ–‡ä»¶å'])
            
            # æŒ‰æ€»æ—¶é•¿æ’åºï¼ˆä»é•¿åˆ°çŸ­ï¼‰
            sorted_stats = sorted(statistics.items(), key=lambda x: x[1]['total_duration'], reverse=True)
            for channels_str, info in sorted_stats:
                writer.writerow([
                    channels_str,
                    info['count'],
                    f"{info['total_duration']:.2f}",
                    f"{info['total_duration']/60:.2f}",
                    info['output_file']
                ])
        
        print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_csv_path}")
        print(f"\né€šé“ç»„ç»Ÿè®¡ (æŒ‰æ€»æ—¶é•¿æ’åº):")
        print(f"{'é€šé“ç»„åˆ':<30} {'ç‰‡æ®µæ•°':<10} {'æ€»æ—¶é•¿(ç§’)':<15} {'æ€»æ—¶é•¿(åˆ†é’Ÿ)':<15}")
        print("-" * 75)
        for channels_str, info in sorted_stats:
            print(f"{channels_str:<30} {info['count']:<10} {info['total_duration']:<15.2f} {info['total_duration']/60:<15.2f}")
        print()
    
    return True


def process_all_results(root_dir):
    """
    é€’å½’å¤„ç†æ‰€æœ‰åŒ…å« marked_abnormal_segments_postICA.csv çš„ç»“æœç›®å½•
    
    å‚æ•°:
        root_dir: æ ¹ç›®å½•
    """
    processed_count = 0
    failed_count = 0
    
    for dirpath, dirnames, filenames in os.walk(root_dir):
        if 'marked_abnormal_segments_postICA.csv' in filenames:
            try:
                success = process_single_result_dir(dirpath)
                if success:
                    processed_count += 1
                else:
                    failed_count += 1
            except Exception as e:
                print(f"âŒ Error processing {dirpath}: {e}")
                failed_count += 1
    
    return processed_count, failed_count


def main():
    parser = argparse.ArgumentParser(
        description="æŒ‰å¼‚å¸¸é€šé“åˆ†ç»„åˆå¹¶æ•°æ®æ®µ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
åŠŸèƒ½è¯´æ˜:
  å°†ç›¸åŒå¼‚å¸¸é€šé“åˆ—è¡¨çš„æ‰€æœ‰æ—¶é—´æ®µåˆå¹¶åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œä¸è€ƒè™‘æ—¶é—´æ˜¯å¦è¿ç»­ã€‚
  ä¾‹å¦‚ï¼š
    - 10-15s: Sph-R,T4
    - 25-30s: Sph-R,T4
    - 40-45s: O1,F7
  
  ä¼šç”Ÿæˆ:
    - xxx_merged_Sph_R_T4.set (åŒ…å« 10-15s å’Œ 25-30s çš„æ•°æ®æ‹¼æ¥)
    - xxx_merged_O1_F7.set (åŒ…å« 40-45s çš„æ•°æ®)

ä½¿ç”¨æ–¹æ³•:
  # å¤„ç†å•ä¸ªç»“æœç›®å½•
  python merge_by_channels.py --result_dir "E:\\data\\patient\\SZ1_results"
  
  # æ‰¹é‡å¤„ç†æ‰€æœ‰ç»“æœç›®å½•
  python merge_by_channels.py --root_dir "E:\\DataSet\\EEG\\EEG dataset_SUAT_processed"

æ³¨æ„:
  éœ€è¦å…ˆè¿è¡Œ process_marked_segments.py ç”Ÿæˆ marked_abnormal_segments_postICA.csv
        """
    )
    
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        '--root_dir',
        help="æ ¹ç›®å½•ï¼Œé€’å½’å¤„ç†æ‰€æœ‰åŒ…å« marked_abnormal_segments_postICA.csv çš„ç›®å½•"
    )
    group.add_argument(
        '--result_dir',
        help="å•ä¸ªç»“æœç›®å½•è·¯å¾„ï¼ˆåŒ…å« marked_abnormal_segments_postICA.csvï¼‰"
    )
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("æŒ‰å¼‚å¸¸é€šé“åˆ†ç»„åˆå¹¶æ•°æ®æ®µ")
    print("=" * 80)
    
    if args.root_dir:
        print(f"\næ‰«æç›®å½•: {args.root_dir}\n")
        processed_count, failed_count = process_all_results(args.root_dir)
        
        print("\n" + "=" * 80)
        print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼")
        print(f"   æˆåŠŸ: {processed_count} ä¸ªç›®å½•")
        if failed_count > 0:
            print(f"   å¤±è´¥: {failed_count} ä¸ªç›®å½•")
        print("=" * 80)
    
    elif args.result_dir:
        print(f"\nå¤„ç†å•ä¸ªç›®å½•: {args.result_dir}\n")
        success = process_single_result_dir(args.result_dir)
        
        if not success:
            print("\nâŒ å¤„ç†å¤±è´¥")
            return 1
    
    return 0


if __name__ == "__main__":
    # å¦‚æœä¸æä¾›å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
    import sys
    if len(sys.argv) == 1:
        print("ä½¿ç”¨é»˜è®¤å‚æ•°è¿è¡Œ...")
        sys.argv.extend([
            '--root_dir', r'E:\DataSet\EEG\EEG dataset_SUAT_processed'
        ])
    
    sys.exit(main())

